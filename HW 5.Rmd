---
title: "HW: Week 5"
author: "36-350 -- Statistical Computing"
date: "Week 5 -- Spring 2020"
output:
  pdf_document:
    toc: no
  html_document:
    toc: true
    toc_float: true
    theme: spacelab
---

Name: Edwin Baik

Andrew ID: ebaik

This HW is to be begun in class, but may be finished outside of class at any time prior to Friday, February 14<sup>th</sup> at 6:00 PM. You must submit **your own** lab as a knitted PDF file on Gradescope.

```{r wrap-hook,echo=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
```



```{r,messages=FALSE}
suppressWarnings(library(tidyverse))
```



## Question 1
*(12 points)*

An alternative to `read.table()` and such is the `scan()` function. The `scan()` function is *very* handy, particularly when someone gives you weirdly formatted text data files. (Maybe groups of unequal-length rows map to one record, etc., etc.) In this problem, use `scan()` to read in `simple.txt` (which you downloaded for Lab 5) and then post-process what you've read in to create a data frame with correct column names and correct data types (`character` for the `name` column and `double` for all the other columns). Your final step will be to print out the data frame. Look at the documentation for `scan()` and pay particular attention to the `what` argument. Once you've scanned the data, use a combination of, e.g., `matrix()` and `data.frame()` to bend the data to your will, and then cast the data in columns 2 through 8 to `numeric`. Hint: `t()` transposes a matrix. Also, pass `stringsAsFactors=FALSE` as an argument to `data.frame()`.
```{r linewidth=80}
simple.scan = scan(file = "simple.txt", 
what = list("character","numeric","numeric","numeric","numeric","numeric","numeric","numeric"))
output = t(matrix(unlist(simple.scan), ncol = 10, byrow = TRUE))
output = output[-1,]
col.names = c("name","u","g","r","i","z","y","redshift")
colnames(output) = col.names
data.frame(output, stringsAsFactors = F) %>% 
  mutate_at(., c("u","g","r","i","z","y","redshift"), as.numeric)
```

## Question 2
*(12 points)*

Let's up the ante a bit here. Download `branch.txt` from the `DATA` directory on Canvas. Examine it with an external viewer. This one's a bit of a mess. (Welcome to real-world data.) Construct a data frame from these data. Assume all the columns are character (there is no need in this exercise to do a final cast of the numeric columns to numeric type). To read in the data themselves, I'd advise you to use `scan()` while skipping the first line and using "|" as the separator. (See the documentation for `scan()`.) To make the data frame, you could use a combination of `matrix()` and `data.frame()` as in Q1, but before doing do, clean up your strings: replace all tab symbols (`\t`) with empty strings, and replace any leading spaces and trailing spaces with empty strings. (Hint: `gsub()`.) Note that the data comprise 14 columns and 39 rows (not including the header). 

Getting the column names is a bit trickier: they are separated by `|_.`, which `scan()` cannot handle. So I'd advise you to use `scan()` to read in *just the first line* (use `\n` as a separator; see the argument `n`), then use `strsplit()` to split the line into 14 column names. You might have to "escape" (i.e., apply double backslashes) some or all of the characters used in splitting. Again, clean things up: get rid of `\t` symbols and trailing spaces.

In the end, display the first four columns and first six rows of your beautiful data frame, rising like a phoenix from the ashes of the terribly formatted ASCII file that you began with.
```{r linewidth=80}
branch.scan = scan(file = "branch.txt", what = character(), sep = "|", skip = 1)
branch.scan = gsub("\t", "", branch.scan) %>% trimws(.,)
branch.output = matrix((branch.scan), ncol = 14, byrow = TRUE)

first.branch = scan(file = "branch.txt", what = character(), sep = "\n", nmax = 1)
first.branch = strsplit(first.branch, split = "\\|_\\.")
col.branch.names = gsub("\t", "", unlist(first.branch)) %>% 
                   gsub("\\|", "", .) %>% 
                   trimws(.,) 
col.branch.names = append(col.branch.names, "Nothing")

colnames(branch.output) = col.branch.names

data.frame(branch.output, stringsAsFactors = F) %>% 
  select(., Subm_ID, Score, Sigma_s, Detection_image) %>% head(.) 
```

## Question 3
*(14 points)*

Read in data from `https://download.bls.gov/pub/time.series/ap/ap.data.0.Current`, which are housed at the Bureau of Labor Statistics. Note before you start that the data are *tab delimited*, and you might find it helpful to remember that a tab is denoted `\t` in a string. The data may not read in cleanly with a simple function call; you may need to skip the header, in which case you will need to provide column names yourself. Also, the parser may misidentify column types, so you may have to set those too. And...you may have to cast data in some columns to be of proper type, after the reading in of the data is done. (Data wrangling is a messy business.) Once everything is read in and cast to (if necessary) proper type, display the mean and standard deviation of the data in the value column for every year *after* 2009 (i.e., 2010 and later). The tidyverse will help you here. Hint: `group_by()`.
```{r linewidth=80}
current.data = read_delim("https://download.bls.gov/pub/time.series/ap/ap.data.0.Current", 
                          delim = "\t")

mean.after.2009 = mutate_at(current.data, c("       value"), as.numeric) %>% 
                  group_by(year)%>% 
                  summarize(avg.value = mean(`       value`, na.rm = T)) %>% 
                  filter(., (year > 2009))
mean.after.2009

sd.after.2009 = mutate_at(current.data, c("       value"), as.numeric) %>% 
                group_by(year) %>% 
                summarize(sd.value = sd(`       value`, na.rm = T)) %>% 
                filter(., (year > 2009))
sd.after.2009
```

## Question 4
*(12 points)*

Download `planets.csv` from the Canvas site. It is in the Week 7 directory. Use an external viewer (your choice) to look at the file. Then apply an appropriate function to read the file's contents into `R`. Your goal: to determine what proportion of the columns have data in at least 20% of their rows. (In other words, step from column to column and see if the proportion of `NA`'s is less than 80%. Then determine the proportion of the columns that fulfill this condition.) Your final answer should be 82.86% [or 0.8286].
```{r linewidth=80}
planet.data = read_csv("planets.csv", skip = 73)
na.check = apply(planet.data, 2, function(x) sum(is.na(x)))
sum(na.check < (0.8*3550)) / length(na.check)
```

## Question 5
*(14 points)*

Make a data frame that is in essence a "dictionary" for the data in the `planets.csv` file. What this means is: extract those lines of the file that contain variable names and corresponding definitions, and from those lines extract the variable names into a vector called `variable` and the definitions into a vector called `definition`. Output the first six rows only! (Hint: in your call to `data.frame()`, set the argument `stringsAsFactors` to `FALSE`. This changes the column contents to character strings rather than factor variables.) Hint: let's say you do an `strsplit()` to split the variable from the definition in each line. The output will be a list, with one list element for each line that contains two strings, one for the variable and one for the definition. A handy way to extract all of the variables would be, e.g., <tt>sapply(&lt;output from strplit&gt;,`[[`,1)</tt>. That `[[` function is really useful.
```{r linewidth=80}
planet.def = read.csv("planets.csv", stringsAsFactors = F) %>% slice(c(3:71)) %>% .[[1]] 
name.check = strsplit(substr(planet.def,10, nchar(planet.def)), split = ":")
var.def.vec = trimws(unlist(name.check))

variable = var.def.vec[c(TRUE,FALSE)]
definition = var.def.vec[c(FALSE,TRUE)]

data.frame(cbind(variable,definition)) %>% head(.)
```

## Question 6
*(12 points)*

Extract the 2019 Major League Baseball standings from the web site given below and put them into a *single* data frame that contains all 30 MLB teams, with the first column being the team name, the second column being the number of wins, and the third column being the number of losses. Order the data frame by decreasing number of wins. (The Houston Astros should be in the first row, and the Detroit Tigers should be in the last one.) Use `rvest` functions to extract any tables you need, which are of class `data.frame`, and then process the data frames until you get a single one as described above. (Note: some team names will have codes in front of them, like "z-". Don't worry about these: they may be included with the output. Also, you might find it useful to set the row names of your final data frame to `NULL`.)
```{r linewidth=80}
if ( require(rvest) == FALSE ) {
  install.packages("rvest",repos="https://cloud.r-project.org")
  library(rvest)
}

site = read_html("http://yesnetwork.sportsdirectinc.com/baseball/mlb-standings.aspx?page=/data/mlb/standings/2019/league/standings.html")
al.df = site %>% html_nodes("table") %>% .[1] %>% html_table()
al.df = select(al.df[[1]],X1,X2,X3)

nl.df = site %>% html_nodes("table") %>% .[2] %>% html_table()
nl.df = select(nl.df[[1]],X1,X2,X3)

team.df = rbind(al.df,nl.df) %>% slice(.,c(2:6,8:12,14:18,20:24,26:30,32:36))
names(team.df) = c("Team", "Wins", "Losses")
team.df %>% mutate_at(.,c("Wins","Losses"), as.numeric) %>% 
arrange(., desc(Wins))
```

## Question 7
*(12 points)*

Your goal: to create a data frame with statistics course id's in one column (e.g., "36-350") and associated course titles in another (e.g., "Statistical Computing"). Google "cmu course catalog statistics" and go to the course catalog page. (Note: this is *not* the course schedule page.) The page should be entitled "Department of Statistics and Data Science Courses". View the HTML source and see if there is a single node delimiter that will return to you the individual course descriptions. (Hint: there is one.) Use string manipulation tools to extract the course id and course title only from the output of `html_text()`, and then place each into their own data frame columns. Display the final data frame. (Hint: you may need to employ the `[[` function here. See Q6.) (Hint 2: `substr()` might prove handy at the final string manipulation steps, handier than `strsplit()`.) As usual, in your call to `data.frame()`, set the argument `stringsAsFactors` to `FALSE`. Set the column names in your data frame to be "ID" and "Title". Display the first ten rows only.
```{r linewidth=80}
page = read_html("http://coursecatalog.web.cmu.edu/schools-colleges/dietrichcollegeofhumanitiesandsocialsciences/departmentofstatistics/courses/")
id.title.vec = page %>% html_nodes("dt") %>% html_text()
id.vec = substr(id.title.vec,1,6)
title.vec = substr(id.title.vec, 8, nchar(id.title.vec))

res.df = data.frame(cbind(id.vec,title.vec),stringsAsFactors = F)
names(res.df) = c("ID", "Course Title")
head(res.df, 10)
```

## Question 8
*(12 points)*

Here you will build upon Q17 of Lab 5 and plot CMU's tuition for every year from 2000 to 2017 (18 values total). At this point in the course, I'm not going to be particularly picky about how you construct your data-extraction "engine"; for instance, you may find it easiest to run a `for()` loop after having set aside a vector called `tuition`. Or maybe you'll find it intuitive to use pipes. Do whatever works for you. Just get that plot built! Apply proper labeling to both the x and y axes, add a title to the plot, and set `pch=19`. Set the y-axis limits to be 0 to the maximum value along the y-axis. And add some color.
```{r linewidth=80}
if ( require(httr) == FALSE ) {
  install.packages("httr",repos="https://cloud.r-project.org")
  library(httr)
}
if ( require(jsonlite) == FALSE ) {
  install.packages("jsonlite",repos="https://cloud.r-project.org")
  library(jsonlite)
}

URL = "https://api.data.gov/ed/collegescorecard/v1/schools?"
key = "eElLbMkcJ8pf1flU0HPUblYWHtFxEgF8UXsUOQrk"

response = GET("https://api.data.gov/ed/collegescorecard/v1/schools?", 
               query = list(api_key = key, 
                            school.name = "Carnegie Mellon University"))

res.text = fromJSON(content(response,"text"),simplifyVector = FALSE)  %>% 
  .$results %>% .[[1]]

year.vec = c("2000","2001","2002","2003","2004","2005","2006","2007","2008",
             "2009","2010","2011","2012","2013","2014","2015","2016","2017")

tuition.vec = vector()
for (val in year.vec) {
   tuition.val = res.text %>% .[[val]] %>% .$`cost` %>% .$`tuition` %>% .[2] %>% 
                 .$in_state
   tuition.vec = append(tuition.vec, tuition.val)
  }

plot(year.vec, tuition.vec, 
     pch = 19, 
     main = "Tuition vs Year", 
     col = "pink",
     ylab = "Tuition",
     xlab = "Year",
     ylim = c(0,max(tuition.vec)))
```