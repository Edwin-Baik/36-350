---
title: "HW: Week 6"
author: "36-350 -- Statistical Computing"
date: "Week 6 -- Spring 2020"
output:
  pdf_document:
    toc: no
  html_document:
    toc: true
    toc_float: true
    theme: spacelab
---

Name: Edwin Baik

Andrew ID: ebaik

This lab is to be begun in class, but may be finished outside of class at any time prior to Friday, February 21<sup>st</sup> at 6:00 PM. You must submit **your own** lab as a knitted PDF file on Gradescope.

```{r wrap-hook,echo=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
```



```{r}
suppressWarnings(library(tidyverse))
```



## Question 1
*(10 points)*

Create a Gaussian mixture model sampler. In this sampler, a datum has a 60% chance of being sampled from a N(-1,1) distribution, and a 40% chance of being sampled from a N(2,1/9) distribution. Sample 100,000 data and create a density histogram of your result. Hint: use `sample()` with `replace` set to `TRUE` and an appropriate vector for `prob` in order to determine which of your 100,000 data should randomly be assigned to distribution 1 as opposed to distribution 2. Also note that if you create a sample of data from distribution 1 and another sample from distribution 2, you can simply combine them by doing, e.g., `x = c(sample1,sample2)`, where `x` becomes a vector of length 100,000.
```{r linewidth=80}
set.seed(808)
poss = c(0, 1)
data = sample(x = poss, size = 100000, replace = TRUE, prob = c(.60,.40))

sample.func = function(x) {
  if (x == 0) {
    x = rnorm(1, mean = -1, sd = 1)
    return(x)
  }
  if (x == 1) {
    x = rnorm(1, mean = 2, sd = 1/9)
    return(x)
  }
}

data.df = as.data.frame(data)
data.df$data = apply(data.df, 1, sample.func)
ggplot(data.df, mapping  = aes(x = data)) + 
  geom_histogram(aes(y=..density..), color = "blue")
```

## Question 2
*(10 points)*

What is the mean of the mixture model in Q1? Compute this via importance sampling, with 100,000 sampled points. You should get an answer around 0.2 (which you can actually derive analytically: if $X \sim N(-1,1)$ and $Y \sim N(2,1/9)$, then $E[0.6X+0.4Y]) = 0.6E[X] + 0.4E[Y] = -0.6 + 0.8 = 0.2$).
```{r linewidth=80}
mean(data.df$data)
```

## Question 3
*(10 points)*

Remember the Chutes and Ladders question? (Q4 of HW 2.) Display a probability histogram that shows the empirical PDF for the number of spins, computed over 10,000 Chutes and Ladders games. Also display the average number of spins that it takes to win the game (approximately 39, give or take) and the minimum number of spins. (Display these two numbers using `cat()`, being see to indicate which is the mean and which is the minimum number of spins.) (Free feel to use your code from HW 2 as a base for what you do here.)
```{r linewidth=80}
set.seed(404)
ladder.bottom = c(1,4,9,21,28,36,51,71,80)
ladder.top = c(38,14,31,42,84,44,67,91,100)
chute.bottom = c(6,26,11,53,19,60,24,73,75,78)
chute.top = c(16,47,49,56,62,64,87,93,95,98)
sample.cl = rep(0,10000)
chute.ladd.func = function(x) {
 position = 0
 count = 0
 while (position != 100) {
  roll = sample(1:6,1)
  if (position + roll > 100) { 
    count = count + 1
    next 
  }
  w = which(ladder.bottom == position + roll)
  if (length(w) > 0) {
    position = ladder.top[w]
    count = count + 1
    next
  }
  l = which(chute.top == position + roll)
  if (length(l) > 0) {
    position = chute.bottom[l]
    count = count + 1
    next
  }
  else {
    position = position + roll
    count = count + 1
  }
 }
 return(count)
}
roll.count = as.data.frame(sample.cl) %>% apply(.,1,chute.ladd.func)
roll.count.df = as.data.frame(roll.count)
ggplot(roll.count.df, mapping  = aes(x = roll.count)) + 
geom_histogram(aes(y=..density..), color = "blue")

cat(mean(roll.count), min(roll.count))
```

## Question 4
*(10 points)*

You are given the following distribution:
$$
f(x) = \frac{4}{11}(x^3+3x+1)~~~x \in [0,1]
$$
It looks like this:
```{r linewidth=80}
x = seq(0,1,by=0.001)
fx = 4*(x^3+3*x+1)/11
ggplot(data=data.frame(x=x,y=fx),mapping=aes(x=x,y=y)) + 
  geom_line(col="peru",size=2) + ylim(0,max(fx))
```

Code up an inverse transform sampler that allows you to efficiently sample 1000 data from this distribution. Initially you will work with this and say "but...I cannot easily invert the CDF, because it's a quartic and all." To which I say "`polyroot()`, which will give you one real root between 0 and 1." To which you will say, "how do I extract that real root?" To which I will say "If you save the output of `polyroot()` as `p`, then the real roots are given by `w = which(abs(Im(p))<1.e-6`." (The 1.e-6 is a check against round-off error.) You then determine which value of `Re(p)[w]` is within the pdf bounds. Histogram your sample with the function line overlaid, and save your sample as `sample.it`. Note: pass a new argument to your histogram, `breaks=seq(0,1,by=0.05)`.
```{r linewidth=80}
set.seed(101)
f.df = data.frame("x"= x,"fx"= fx)

N = 1000
u = runif(N)
c = rep(0,1000)

for (ii in 1:1000) {
  p = polyroot(c(-u[ii],4/11,6/11,0,1/11))
  w = which(abs(Im(p)) < 1.e-6)
  c[ii] = Re(p[w])[(Re(p[w]) <= 1 & Re(p[w]) >= 0)]
}

sample.it = c

ggplot(as.data.frame(sample.it), mapping = aes(x = sample.it)) + 
  geom_histogram(aes(y=..density..),color = "blue", breaks = seq(0,1,by=0.05)) +
  geom_line(mapping=aes(x = x, y = fx),data = f.df,col = "red", size=0.7)
```

## Question 5
*(10 points)*

Code up a rejection sampler that allows you to also sample 1000 data from this pdf given in the last question. Again, histogram your sample and overlay the pdf. Save your sample as `sample.rs`.
```{r linewidth=80}
set.seed(101)
f.df = data.frame("x"= x,"fx"= fx)

k = 1000
sample.rs = rep(NA, k)

f.x = function(y) {
  z = (4*(y^3+3*y+1)/11)
  return (z)
}

index = 1
while (index <= k) {
  sample.rs[index] = runif(1,min=0,max=1)            
  if (runif(1, min=0, max = f.x(1)) <= (f.x(sample.rs[index]))) { 
    index = index + 1   
  }
}

sample.df = as.data.frame(sample.rs)

ggplot(sample.df, mapping = aes(x = sample.rs)) + 
  geom_histogram(aes(y=..density..),color = "blue") +
  geom_line(mapping=aes(x = x, y = fx),data = f.df,col = "red", size=0.7)
```

## Question 6
*(10 points)*

Test the hypothesis that `sample.it` and `sample.rs` are both sampled from the same parent population. (I mean they are, but...) Either recall how you would do a two-sample test or Google how you would do it. (Note: I'm not talking about a two-sample t-test here! We are not testing the hypothesis that the distribution means are the same. We are testing the hypothesis that both samples are drawn from the same underlying population.) There are various options for doing this; pick one, and display the p-value. If it less than 0.05, we reject the null. (Hint: it shouldn't be.) In addition, plot the empirical cdfs for both samples; see the documentation for `ecdf()` for help. (To be clear: use the base `R` function `plot()` here, and not `ggplot()`.) Note that to plot a second ecdf on top of the first, you need to call `plot()` a second time, with the argument `add=TRUE`.
```{r linewidth=80}
ks.test(sample.it, sample.rs)$p.value

sampleit = ecdf(sample.it)
samplers = ecdf(sample.rs)

plot(sampleit, col = "blue")
plot(samplers, col = "red", add = TRUE)
```

## Question 7
*(10 points)*

Write an inverse transform sampler that samples 10,000 data from a exponential distribution with rate parameter 1, but it keeps over the ranges $[0.5,1]$ and $[2,4]$. Make a probability histogram of your result. This time, tweak the call to `geom_histogram()` by adding the argument `breaks=seq(0,5,by=0.1)`.

This is a bit tricky. (Note: this is an inverse transform sampler, so every single randomly sampled uniform random variable has to get mapped to a valid value of $x$.) You might want to start by computing the cdf of the distribution up to the points 0.5, 1, 2, and 4, and go from there. You can compute the cdf analytically! Note: to map from your uniform random variables to exponentially distributed ones, pass your uniform r.v.'s into `qexp()`.
```{r linewidth=80}
set.seed(404)
k = 10000

p.lower = (dexp(1)-dexp(0.5))/((dexp(1)-dexp(0.5)) + (dexp(4) - dexp(2)))
p.upper = 1 - p.lower

f = function(x) {
  return (1 - exp(-x))
}

f1 = function(x) {
  r = (f(1) - f(0.5)) / p.lower
  y = -log(1 - (x * r + f(0.5)))
  return (y)
}

f2 = function(x) {
  r = (f(4) - f(2)) / p.upper
  b = x - p.lower
  y = -log(1 - (b * r + f(2)))
  return (y)
}

u = runif(10000)
data = sapply(u, function(x) {if (x < p.lower){f1(x)} else{f2(x)}})


ggplot(as.data.frame(data), mapping  = aes(x = data)) + 
  geom_histogram(aes(y=..density..), color = "blue")

```

## Question 8
*(10 points)*

And now for something completely different: randomly sampling a name for your new baby, given 1930 Social Security Administration data. (You did know you were having a baby, right?)

You'll see the `babynames` tibble has five columns. Using `dplyr` techniques, extract the rows for which the year is 1930, then create a new column that shows the cumulative sum of the `prop` column. Then save the names and the cumulative sum column to a new table. Pick a random number between 0 and 1, and pick the first row for which the cumulative sum is greater than that number. (If you have three rows for which the cumulative sums are 0.55, 0.8, and 1, and you sample 0.67, you'd pick the name on the *second* row, since 0.8 is the first cumulative sum greater than 0.67.) Display your random number, and the full contents of that last row.

My kid's apparently going to be named Anne, by the way.
```{r linewidth=80}
if ( require(babynames) == FALSE ) {
  install.packages("babynames",repos="https://cloud.r-project.org")
  library(babynames)
}
set.seed(404)
bn.year.df = babynames %>% group_by(year) %>% filter(., (year == 1930)) %>% 
             mutate(., prop.sum = cumsum(prop))

x = runif(1,0,1)
x
bn.year.df[which(bn.year.df$prop.sum > x)[1],]
```

## Question 9
*(10 points)*

Numerically estimate the median of the pdf
$$
f(x) = \frac{2.92959}{\sqrt{2\pi}}e^{-x^2/2}~~~x \in [0,1] \,.
$$
(This is a truncated normal distribution.) The median is the value $y$ such that
$$
\int_0^y f(x) dx = 0.5 \,.
$$
A not-elegant way to do this is to use `integrate()` over and over again until you hone in on an integral value of 0.5. Don't do this. A more elegant solution is to determine, via `uniroot()`, the root of the function
$$
g(y) = \left( \int_0^y f(x) dx \right) - 0.5 \,,
$$
i.e., the value of $y$ such that $g(y) = 0$. Do do this.
```{r linewidth=80}
g.y = function(y) {
  as.numeric(integrate(f.x,0,y)["value"]) - 0.5
}

f.x = function(x) {
  2.92959/(sqrt(2*pi)) * exp(-x^2/2)
}

integral = as.numeric(uniroot(g.y,c(0,1))["root"])
integral
integrate(f.x, 0, integral)

```

## Question 10
*(10 points)*

The ratio of the area of a circle to the area of a square into which the circle is inscribed is $\pi/4$. Does this ratio increase or decrease with dimensionality? For instance, what is the ratio of volume of a sphere to the volume of a cube into which the sphere is inscribed? Is it less than $\pi/4$? Compute (and display) the ratio for dimensions 3, 4, ..., 10. The result that you see has manifestations for, e.g., algorithms based on nearest neighbors, etc. Curse of dimensionality, n'at. (Hint: to do this calculation succinctly, consider putting samples from your uniform distribution into a $k \times d$ matrix, where $k$ is the number of sampled points, and $d$ is the dimensionality. Then you can use `apply()` to determine the distance of the points from the origin and you can easily finish the calculation from there...)
```{r linewidth=80}
set.seed(404)
K = 100000
sum = 0
f.distance = function(x,d) {
  return (sqrt(sum(x[1:d])))
}

f.dist.check = function(x,d) {
  return (f.distance(x,d) <= 1)
}
sample.vec = rep(NA,K)

sample.mat = matrix(sapply(sample.vec, function(x){runif(1,-1,1)^2}), nrow = K/10)

count.vec = apply(sample.mat, 1, function(x){f.dist.check(x,3)})

for (i in 3:10) {
  count.vec = apply(sample.mat, 1, function(x) {f.dist.check(x,i)})
  cat(i, pi/(sum(count.vec)/(K/10)), "\n")
}

```