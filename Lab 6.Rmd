---
title: "Lab: Week 6"
author: "36-350 -- Statistical Computing"
date: "Week 6 -- Spring 2020"
output:
  pdf_document:
    toc: no
  html_document:
    toc: true
    toc_float: true
    theme: spacelab
---

Name: Edwin Baik 

Andrew ID: ebaik

This lab is to be begun in class, but may be finished outside of class at any time prior to Thursday, February 20<sup>th</sup> at 6:00 PM. You must submit **your own** lab as a knitted PDF file on Gradescope.

```{r wrap-hook,echo=FALSE}
library(knitr)
hook_output = knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})
```



```{r}
suppressWarnings(library(tidyverse))
```



# Simulation

## Question 1
*(5 points)*

*Notes 6A (3-5) and Notes 6C (5)*

What is the distribution of the sample standard deviation for 10 draws from a standard normal? Create an empirical distribution by repeating the process of sampling 5000 times. Make a density histogram of your result using `ggplot`. (Pass the argument `aes(y=..density..)` to `geom_histogram()` to change the default frequency histogram into a density histogram). Test the hypothesis that the empirical distribution is itself normal. (Display the p-value.) Google to find an appropriate test of normality. Be sure to set random number seeds here and below for reproducibility. (Also note that `ggplot()` expects a data frame as input, not a vector; you should simply create a one-column data frame from your vector of sample standard deviations and pass that into `ggplot()`.)
```{r linewidth=80}
set.seed(124)
n.obs = 5000
n.data = 10
data = matrix(rnorm(n.data*n.obs), ncol = n.data)
sd.vector = apply(data, 1, sd)
sd.df = as.data.frame(sd.vector)
ggplot(sd.df, mapping  = aes(x = sd.vector)) + geom_histogram(aes(y=..density..))
shapiro.test(sd.vector)
```
```
The distribution of 10 draws from a standard normal done 5000 times is itself a 
normal distribution, and when a statistical test is done, the p-value created is 
3.373e-08, meaning that there is strong evidence of non-normality for the standard
deviations.
```

## Question 2
*(5 points)*

*Notes 6A (3-5) and Notes 6C (5)*

Repeat the last question, but replace the sample standard deviation with the sample mean. Given the p-value, would you say that the distribution of sample means is closer to being normal than the distribution of sample standard deviations?
```{r linewidth=80}
set.seed(124)
n.obs = 5000
n.data = 10
data = matrix(rnorm(n.data*n.obs), ncol = n.data)
mean.vec = apply(data, 1, mean)
mean.df = as.data.frame(mean.vec)
ggplot(mean.df, mapping  = aes(x = mean.vec)) + geom_histogram(aes(y=..density..))
shapiro.test(mean.df$mean.vec)
```
```
The p-value is 0.4833, showing normality for the sample means, unlike the 
sample standard deviations that showed skewedness with its test of normality and 
its p-value.
```

## Question 3
*(5 points)*

*Notes 6A (3-5) and Notes 6C (5)*

What is the probability distribution function for the difference between the maximum value and the median value when you sample nine data from a Uniform(0,1) distribution? Generate an empirical distribution by repeating the process of sampling nine data 5,000 separate times, and record the differences from the maximum and median values. Display a histogram of your result; in addition, display the mean and standard error. The mean should be approximately 0.4.
```{r linewidth=80}
set.seed(124)
n.obs = 5000
n.data = 9
data = matrix(runif(n.data*n.obs), ncol = n.data)
max.vec = apply(data, 1, max)
med.vec = apply(data, 1, median)
max.med.df = as.data.frame(max.vec)
max.med.df$med.vec = med.vec
max.med.df = max.med.df %>% mutate(.,diff = max.vec - med.vec)
ggplot(max.med.df, mapping = aes(x = diff)) + geom_histogram(color = "blue")
mean(max.med.df$diff)
sd(max.med.df$diff)/sqrt(n.obs)
```

## Question 4
*(5 points)*

*Notes 6A (2-5)*

Estimate the mean and standard error for the sum of three rolled fair dice using 5,000 simulated rolls. Note that there are potentially several ways by which you might attack this problem; how you go about it is up to you, as long as you get a valid final answer. (Which should be close to 10.5.)
```{r linewidth=80}
set.seed(124)
n.obs = 5000
dice = c(1,2,3,4,5,6)
data = matrix(sample(x = dice, size = 3*5000, replace = TRUE), ncol = 3)
sum.vec = apply(data, 1, sum)
mean(sum.vec)
sqrt(var(sum.vec)/length(sum.vec))
```



You are given the following probability density function:
$$
f(x) = \frac{x^2}{9} ~~,~~ x \in [0,3]
$$
It looks like this:
```{r linewidth=80}
x = seq(0,3,by=0.01)
fx = x^2/9
df.pdf = data.frame("x"=x,"fx"=fx)
ggplot(data=df.pdf,mapping=aes(x=x,y=fx)) + geom_line(col="blue",size=2)
```

## Question 5
*(10 points)*

*Notes 6A (6-7) and Notes 6C (5)*

Code up an inverse transform sampler that allows you to efficiently sample 200 data from this pdf. Histogram your sample, and overlay the line showing the pdf. To do this in `ggplot`, you would use a structure like `ggplot(...) + geom_histogram(...) + geom_line(...)`. The main issue is that the data are histogramming are not the data you would be passing to the line. So: do the `ggplot(...) + geom_histogram(...)` in the same manner you did in previous questions above, where the data frame you point to is the one containing your sampled points, then, when you add on `geom_line(...)`, specify a data argument that points to the `df.pdf` variable defined above and specify a mapping argument that refers to columns of `df.pdf` (and add on a color argument and a size argument like we did above, if you wish).
```{r linewidth=80}
set.seed(101)
u = runif(200)
data.points = (27*u)^(1/3)
x.df = as.data.frame(data.points)

ggplot(x.df, mapping = aes(x = data.points)) + 
  geom_histogram(aes(y=..density..),color = "blue") +
  geom_line(mapping = aes(x = df.pdf$x, y = df.pdf$fx), data = df.pdf, size = 1)
```

## Question 6
*(10 points)*

*Notes 6A (8-9) and Notes 6C (5)*

Repeat the previous question, but utilize rejection sampling.
```{r linewidth=80}
set.seed(101)
k = 200
val.vec = rep(NA, k)
index = 1
while (index <= k) {
  val.vec[index] = runif(1,min=0,max=3)            
  if (runif(1,min=0,max=1) < (val.vec[index])^2/9 ) { 
    index = index + 1   
  }
}
val.df = as.data.frame(val.vec)
ggplot(val.df, mapping = aes(x = val.vec)) + 
  geom_histogram(aes(y=..density..),color = "blue") +
  geom_line(mapping = aes(x = df.pdf$x, y = df.pdf$fx), data = df.pdf, size = 2)
```

## Question 7
*(5 points)*

*Notes 6A (3-5)*

In 36-225, you learned that an effective rule of thumb regarding the Central Limit Theorem is that if $n \geq 30$, the mean of your sample is at least approximately normally distributed. Let's test this out. Construct repeated samples of 30 data from an Exponential(1) distribution and record the means: are they normally distributed? Simply show the p-value from an appropriate hypothesis test. If it is $\ll$ 0.05, we'll conclude the rule of thumb doesn't really hold in this instance.
```{r linewidth=80}
set.seed(124)
n.obs = 5000
n.data = 30
data = matrix(rexp(n.data*n.obs), ncol = n.data)
mean.vec = apply(data, 1, mean)
mean.df = as.data.frame(mean.vec)
shapiro.test(mean.df$mean.vec)
```

## Question 8
*(5 points)*

*Notes 6A (3-5)*

Repeat the last question, but for a Uniform(0,1) distribution.
```{r linewidth=80}
set.seed(124)
n.obs = 5000
n.data = 30
data = matrix(runif(n.data*n.obs), ncol = n.data)
mean.vec = apply(data, 1, mean)
mean.df = as.data.frame(mean.vec)
shapiro.test(mean.df$mean.vec)
```

## Question 9
*(5 points)*

*Notes 6A (2-5)*

You have been called to testify in a trial on the alien planet Slybobia. Prosecutors allege that on this planet, whose intelligent population is 25% blue, 25% green, 25% orange, and 25% fuchsia, it would require bias to construct a panel of 10 Slybobians in which at least 4 are orange *and* at least 4 are fuchsia. But such a panel had been constructed. Your expert testimony is needed: what is the probability that such a panel would be constructed if sampling of the population was truly done randomly? Is it smaller than 0.05? If so, that would indicate that you should reject the null hypothesis that the panel composition is unbiased.
```{r linewidth=80}
set.seed(124)
n.obs = 5000
aliens = c(1,2,3,4)
data = matrix(sample(x = aliens, size = 10*5000, replace = TRUE), ncol = 10)

orange.fuchsia.func = function(x) {
  if (sum(x == 3) | sum(x == 4)) {
    return (TRUE)
  }
  else {
    return (FALSE)
  }
}

orange.vec = apply(data, 1, orange.fuchsia.func)
1 - length(orange.vec[orange.vec == T])/length(orange.vec)
```
```
Here, the p-value is smaller than 0.05, so we can reject the null hypothesis
and say that the panel composition is biased. 
```

## Question 10
*(5 points)*

*Notes 6A (8-9)*

An old classic: what is the value of $\pi$? Use rejection sampling in two dimensions (and, oh, 10,000,000 samples) to estimate $\pi$. (Think of a unit circle inscribed within a 2 $\times$ 2 box centered at the origin. The box will have an area of 4, and the circle will have an area of $\pi$...thus the ratio of the total number of random samples inside the unit circle to the total number of samples will approach $\pi$/4 as the number of samples approaches $\infty$.) Also display the percentage error of your estimate, which you can compute using `R`'s built-in value of $\pi$ (i.e., the `R` constant `pi`.)
```{r linewidth=80}
set.seed(101)
k = 10000000
X = runif(k)
Y = runif(k)
r2 = X^2 + Y^2
pi.estimate = sum(r2 <= 1) / (k/4)
pi.estimate
```

## Question 11
*(10 points)*

*Notes 6A (6-9)*

You are given the following bivariate pdf:
$$
f(x_1,x_2) = \left\{ \begin{array}{cc} 3x_1 & 0 \leq x_2 \leq x_1 \leq 1 \\ 0 & \mbox{otherwise} \end{array} \right.
$$
(This is borrowed from Exercise 5.5 of Wackerly 7.) Code a function for sampling data from this distribution. Sample 5000 $(x_1,x_2)$ pairs, then visualize them with a scatter plot. You should observe a greater density of points as you go from the left to the right.
```{r linewidth=80}
set.seed(404)
N = 5000
x = rep(NA, N)
y = rep(NA, N)

ii = 1

while (ii <= N) {
  x[ii] = runif(1, min = 0, max = 1)
  y[ii] = runif(1, min = 0, max = 1)
  if (y[ii] <= x[ii]) {
    ii = ii + 1
  }
}

ggplot(data.frame(x,y), mapping=aes(x = x, y = y)) + geom_point()

```

# Integration

## Question 12
*(5 points)*

*Notes 6B (3-4)*

Compute the integral
$$
\int_0^3 x^2 e^{-x^4} dx
$$
via Monte Carlo integration, with 100,000 points. You should achieve a value close to 0.30635.
```{r linewidth=80}
set.seed(808)
g.x = function(x) {
  exp(-x^4)*(x^2)
}

x = runif(1000,min=0,max=3) 
w.x = g.x(x)/(1/3)
print(mean(w.x))
```

## Question 13
*(5 points)*

*Notes 6B (3-4)*

Compute the integral
$$
\int_0^1 \int_0^1 \left(\cos\left(\frac{\pi}{2}x_1\right) + x_2^3\right) dx_1 dx_2
$$
via Monte Carlo integration, with 1,000,000 points. Your value should be close to 0.8866.
```{r linewidth=80}
set.seed(808)
g.x = function(x,y) {
  cos(pi/2*x) + y^3
}
x = runif(1000000,min=0,max=1)
y = runif(1000000,min=0,max=1)
w.x.y = g.x(x,y)

print(mean(w.x.y))
```

## Question 14
*(5 points)*

*Notes 6A (8-9) and Notes 6C (3-4)*

Let's change up that last integral just a bit:
$$
\int_0^1 \int_0^{x_2} \left(\cos\left(\frac{\pi}{2}x_1\right) + x_2^3\right) dx_1 dx_2
$$
The region of integration is now the triangle with vertices (0,0), (0,1), and (1,1). (Commence hallucinatory flashbacks to 225, if they haven't started already.) Combine a rejection sampler and MC integration to perform this integral. Sample approximately 1,000,000 points in the region of integration and then use those. (By using the word "approximately," I'm encouraging you to be clever in how you do the sampling...like by sampling 2,000,000 points in a box and keeping the roughly 1,000,000 that lie within the triangular region of integration.) Realize that for a bivariate uniform within the region of integration, $f(x_1,x_2) = 2$. (Your answer should be approximately 0.605.)
```{r linewidth=80}
n = 1000000
s1 = runif(n)
s2 = runif(n)
x2 = s2[s2 > s1]
x1 = s1[s2 > s1]

g.x = function(x,y) {
   cos(pi/2*x) + y^3
}

w = g.x(x1,x2)/2
mean(w)
```

## Question 15
*(10 points)*

*Notes 6C (5-8)*

You are given the following distribution:
$$
f(x) = e^{-x {\rm erf}(x)}/1.140741 ~~~~ x > 0 \,.
$$
"erf(x)" == the error function with input $x$. (You'll need to install and load the `VGAM` package to be able to compute the error function.) Here's a plot of $f(x)$:
```{r linewidth=80}
if ( require(VGAM) == FALSE ) {
  install.packages("VGAM",repos="https://cloud.r-project.org")
  library(VGAM)
}
x = seq(0,6,by=0.01)
fx = exp(-x*erf(x))/1.140741
ggplot(data=data.frame(x=x,fx=fx),mapping=aes(x=x,y=fx)) + geom_line(col="firebrick2",size=2)
```

Use importance sampling to estimate the mean of $f(x)$. Use 100,000 data points. Your result should be approximately 0.95. (Hint: a half-normal distribution with `sigma` about 2.5 makes a nice proposal distribution here. See the `extraDistr` package.)
```{r linewidth=80}
if ( require(extraDistr) == FALSE ) {
  install.packages("extraDistr",repos="https://cloud.r-project.org")
  library(extraDistr)
}
set.seed(808)
N = 100000
x = rhnorm(N, sigma = 2.5)
h = dhnorm(x, sigma = 2.5) # Evaluate h(x)

g = rep(0,N)
g[x > 0] = x[x > 0]    # Evaluate g(x)

f = function(x) {        # Evaluate f(x)
  f.x = rep(0,length(x))
  f.x[x > 0] = exp(-x*erf(x))/1.140741
  return(f.x)
}

mean(g*f(x)/h)
```

## Question 16
*(5 points)*

*Notes 6C (7)*

And now: estimate the standard deviation of $f(x)$. Note: you can reuse many of the variables from above! Your value should again be around 0.95.
```{r linewidth=80}
set.seed(808)
sqrt(mean(x^2*f(x)/h) - mean(x*f(x)/h)^2)
```
